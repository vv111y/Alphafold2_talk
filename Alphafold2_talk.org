* HEADER :ignore:noexport:
#+TITLE: Highly accurate protein structure prediction with AlphaFold
#+SUBTITLE: AISC Healthcare Discussion Group 
#+EMAIL: willy.rempel@rempellabs.com  
#+AUTHOR: Willy Rempel
#+LATEX_HEADER: \author{Willy Rempel}
#+DATE: Saturday, July 24, 2021 
#+DESCRIPTION: 
#+KEYWORDS: 
#+LANGUAGE:  en
#+PROPERTY: header-args :tangle yes :comments link :results link
#+OPTIONS: H:3 toc:nil author:nil todo:nil p:nil stat:nil d:nil num:nil
#+OPTIONS:   TeX:t LaTeX:t skip:nil d:nil todo:t pri:nil tags:not-in-toc
#+INFOJS_OPT: view:nil toc:nil ltoc:t mouse:underline buttons:0 path:https://orgmode.org/org-info.js
#+SELECT_TAGS: export
#+EXCLUDE_TAGS: noexport

#+STARTUP: beamer
#+LATEX_CLASS: beamer
#+LaTeX_CLASS_OPTIONS: [presentation, smaller]
#+COLUMNS: %45ITEM %10BEAMER_ENV(Env) %10BEAMER_ACT(Act) %4BEAMER_COL(Col) %8BEAMER_OPT(Opt)
#+BEAMER_FRAME_LEVEL: 3
#+BEAMER_THEME: Rochester 
#+BEAMER_COLOR_THEME: dolphin
#+BEAMER_HEADER: \graphicspath{{./imgs/}}
#+LATEX_HEADER: \beamertemplatenavigationsymbolsempty
#+LATEX_HEADER: \setbeamertemplate{headline}{}
#+LATEX_HEADER: \setbeamersize{text margin left=0pt,text margin right=0pt}


#+LATEX_HEADER: \usepackage{amsmath, amsthm, amssymb}
#+LATEX_HEADER: \usepackage{verbatim, appendix}
#+LATEX_HEADER: \usepackage{ulem}
#+LATEX_HEADER: \usepackage{graphicx}
#+LATEX_HEADER: \usepackage{caption}
# #+LATEX_HEADER: \usepackage{titletoc}
#+LATEX_HEADER: \usepackage{pseudocode}
#+LaTeX_HEADER: \usepackage[ruled]{algorithm2e}
#+LaTeX_HEADER: \usepackage{array}
# #+LaTeX_HEADER: \usepackage[svgnames, table]{xcolor}
# #+LaTeX_HEADER: \usepackage[most]{tcolorbox}
#+LaTeX_HEADER: \usepackage{booktabs}
#+LaTeX_HEADER: \usepackage{listings}

#+LaTeX_HEADER: \usepackage[]{biblatex} 
#+LaTeX_HEADER: \setbeamertemplate{bibliography item}{\insertbiblabel}
#+LaTeX_HEADER: \AtEveryBibitem{\clearfield{note}}
#+LaTeX_HEADER: \bibliography{AlphafoldTalk2021.bib} 
# bibliography:AlphafoldTalk2021.bib

#+LATEX: \maketitle

\setbeamerfont{large}{size=\large}

* START [0/1] :ignore:noexport:
** LOG
** ---
** [.] train todo 

Predictions of side chain chi angles as well as the final, per-residue accuracy of the structure (pLDDT) are computed with small per-residue networks on the final activations at the end of the network.
The estimate of the TM-score (pTM) is obtained from a pairwise error prediction that is computed as a linear projection from the final pair representation.
The final loss (that we term the frame-aligned point error (FAPE) (Fig. 3f)) compares the predicted atom positions to the true positions under many different alignments.
For each alignment, defined by aligning the predicted frame (Rk,tk) to the corresponding true frame, we compute the distance of all predicted atom positions xi from the true atom positions.
The resulting Nframes × Natoms distances are penalized with a clamped L1-loss.
This creates a strong bias for atoms to be correct relative to the local frame of each residue and hence correct with respect to its side chain interactions, as well as providing the main source of chirality for AlphaFold (Suppl. Methods 1.9.3 and Suppl. Fig. 9).

makes effective use of the unlabelled sequence data and significantly
improves the accuracy of the resulting network.
Additionally, we randomly mask out or mutate individual residues
within the MSA and have a Bidirectional Encoder Representations from
Transformers (BERT)-style37 objective to predict the masked elements
of the MSA sequences. This objective encourages the network to learn
to interpret phylogenetic and covariation relationships without hardcoding a particular correlation statistic into the features. The BERT
objective is trained jointly with the normal PDB structure loss on the
same training examples and is not pre-trained, in contrast to recent
independent work38.
* refs :ignore:noexport:
* snips :ignore:noexport:

- [[https://rempellabs.com][rempellabs.com]] [coming soon] \\
* CODE [0/0] :ignore:noexport:
# * Writeup [0/0] :export:ignore:
* --- :ignore:noexport:
* Results :ignore:noexport:
* Supplemental figures :ignore:noexport:
* Methods :ignore:noexport:
** Model training and evaluation	
* --- :ignore:noexport:
* Introduction 
*** Introduction
**** :B_ignoreheading:BMCOL:
:PROPERTIES:
:BEAMER_COL: 0.4
:END:
#+ATTR_LATEX: :scale 0.3
[[./imgs/profilepic2.jpg]]
**** :BMCOL:
:PROPERTIES:
:BEAMER_COL: 0.6
:END:
Willy Rempel
- HBSc Computer Science \\
- BSc Mathematics \\ 
- Research Associate, AISC \\
- seeking opportunities in the field 
*** Introduction

  #+begin_quote
  Although all of the ideas in the model are doubtlessly clever, the main secret behind AlphaFold 2’s success is the superb deep learning engineering. A close look at the model reveals an architecture with a large amount of small details that seem fundamental for the performance of the network. As we admire the end product, we should not turn a blind eye to the enormous budget, and the large team of full-time, handsomely paid engineers that made it possible.  cite:rubieraAlphaFoldHereWhat
  #+end_quote

  #+begin_quote
  This, and many other tricks, are described in exhaustive detail in the Supplementary Information. A reduced subset has been analysed in a brief ablation study, but ultimately, how important are each of the minor details is anybody’s guess.  cite:rubieraAlphaFoldHereWhat
  #+end_quote

\flushright{(above blog post is recommended reading)}
*** Model Overview cite:jumperHighlyAccurateProtein2021
:PROPERTIES:
:ID:       bef85d4f-05c7-425e-815f-b0698c0ff51a
:END:
#+ATTR_LATEX: width=\textwidth
[[./imgs/model-overview.png]] 

* Data Pipeline
*** Initial Input: mmCIF or FASTA files
#+ATTR_LATEX: height=0.2\textheight
[[./imgs/mmcif-eg.png]]

see cite:PDB101LearnGuide for full guide

*** Initial Input: mmCIF or FASTA files
#+ATTR_LATEX: height=0.9*\textheight
[[./imgs/fastafiles_2021-07-20.png]]

*** Parsing cite:jumperHighlyAccurateProtein2021

- only certain metadata (more from mmCIF)
- change MSE residues into MET

*** Genetic Search cite:jumperHighlyAccurateProtein2021
For MSAs
- JackHMMER
  - MGnify: MSA depth 5,000
  - UniRef90: MSA depth 10,000
- HHBlits
  - Uniclust30 + BFD: MSA depth unlimited
- MSAs duplicated and stacked

flags:
  JackHMMER: -N 1 -E 0.0001 --incE 0.0001 --F1 0.0005 --F2 0.00005 --F3 0.0000005.
  HHBlits: -n 3 -e 0.001 -realign_max 100000 -maxfilt 100000 -min_prefilter_hits 1000 -maxseq 1000000.

*** Template Search cite:jumperHighlyAccurateProtein2021
- UniRef90 MSA from prior search used for PDB70 search using HHSearch.
- Filter out:
  - released after the input sequence
  - or identical to the input sequence
  - too small
- At inference use top 4 templates

*** Training Data cite:jumperHighlyAccurateProtein2021
- 75:25 self-distillation : known structure (PDB)
- stochastic filters (next)

*** Filtering cite:jumperHighlyAccurateProtein2021
- stochastic filters: 
  * Input mmCIFs are restricted to have resolution less than 9 Å. This is not a very restrictive filter and only removes around 0.2% of structures.
  * Longer protein chains are selected with higher probability.
  * Also favour protein chains from smaller clusters. They use 40% sequence identity clusters of the Protein Data Bank clustered with MMSeqs2.
  * Sequences are filtered out when any single amino acid accounts for more than 80% of the input primary sequence. This filter removes about 0.8% of sequences.

*** MSA block deletion cite:jumperHighlyAccurateProtein2021
- MSAs grouped by tool, sorted by block output? (e-value?)
  - similar sequences are likely to be adjacent
  - block deletion tends to remove similarities (ie. whole branch phylogeny)

*** MSA clustering cite:jumperHighlyAccurateProtein2021
- Similarity clusters used to randomly select subset of MSA sequences 
  - to reduce computational cost from attention modules, reduce $N_seq$

1. K-means, input sequence used as first cluster center
2. masking
3. hamming distance measure for remaining selections

*** Residue cropping cite:jumperHighlyAccurateProtein2021
During training:
1. unclamped & clamped - sampling start index from uniform distributions
2. Cropped with fixed size $N_res$

*** Featurization and model inputs cite:jumperHighlyAccurateProtein2021
- *target_feat*
  This is a feature of size [Nres, 21] consisting of the “aatype” feature.
- *residue_index*
  This is a feature of size [Nres] consisting of the “residue_index” feature.
- *msa_feat*
  This is a feature of size [Nclust, Nres, 49] constructed by concatenating “cluster_msa”, “cluster_has_deletion”, “cluster_deletion_value”, “cluster_deletion_mean”, “cluster_profile”. We draw Ncycle×Nensemble random samples from this feature to provide each recycling/ensembling iteration of the network with a different sample (see subsubsection 1.11.2).
- *extra_msa_feat*
  This is a feature of size [Nextra_seq, Nres, 25] constructed by concatenating “extra_msa”, “extra_msa_has_deletion”, “extra_msa_deletion_value”. Together with “msa_feat’ above we also draw Ncycle × Nensemble random samples from this feature (see subsubsection 1.11.2).
*** Featurization and model inputs cite:jumperHighlyAccurateProtein2021
- *template_pair_feat*
  This is a feature of size [Ntempl, Nres, Nres, 88] and consists of concatenation of the pair residue features “template_distogram”, “template_unit_vector”, and also several residue features, which are transformed into pair features. The “template_aatype” feature is included via tiling and stack- ing (this is done twice, in both residue directions). Also the mask features “template_pseudo_beta_mask” and “template_backbone_frame_mask” are included, where the feature fij = maski · maskj. - template_angle_feat This is a feature of size [Ntempl, Nres, 51] constructed by concatenating the following features: “template_aatype”, “template_torsion_angles”, “template_alt_torsion_angles”, and “template_torsion_angles_mask”. 

*** Self-distillation dataset cite:jumperHighlyAccurateProtein2021

- Build dataset (on unlabeled sequences):
  1. Make MSA for every cluster in Uniclust30
  2. Remove sequences that appear in another sequences MSA
  3. Keep sequences of 200 < length < 1024
  4. Remove sequences where MSA < 200 alignments
- For predicted structures:
  - train 'undistlled' model on just PDB dataset
  - use this model to predict above set
  - for every residue pair, computer confidence metric using KL-divergence between distance distribution and a reference distribution
  - reference distribution
- self-distillation training took ~2 weeks

* Model Architecture	
*** Input embeddings cite:jumperHighlyAccurateProtein2021 
#+ATTR_LATEX: :height \textheight
[[./imgs/input_embeddings.png]]
cite:jumperHighlyAccurateProtein2021
** EvoFormer
*** EvoFormer: Overview cite:jumperHighlyAccurateProtein2021
#+ATTR_LATEX: width=\textwidth
[[./imgs/model-evoformer-main.png]] 

*** EvoFormer: Overview cite:jumperHighlyAccurateProtein2021
- cast as a graph inference problem
- cross-optimization and information flow between MSA representation and pair-wise representation
- layer normalization

*** EvoFormer: Row wise Gated Attention cite:jumperHighlyAccurateProtein2021
#+ATTR_LATEX: width=\textwidth
[[./imgs/rowwise-gated-attention.png]]

*** EvoFormer: Column wise Gated Attention cite:jumperHighlyAccurateProtein2021
#+ATTR_LATEX: width=\textwidth
[[./imgs/columnwise-gated-attention.png]]

*** EvoFormer: MSA Translation Layer cite:jumperHighlyAccurateProtein2021
#+ATTR_LATEX: width=\textwidth
[[./imgs/msa-translation-layer.png]]

*** EvoFormer: Outer-Product Mean cite:jumperHighlyAccurateProtein2021
#+ATTR_LATEX: width=\textwidth
[[./imgs/outer-product-mean.png]]

*** EvoFormer: Residue Pairs cite:jumperHighlyAccurateProtein2021
#+ATTR_LATEX: :scale 0.25
[[./imgs/model-evoformer-pair1.png]]
#+ATTR_LATEX: width=\textwidth
[[./imgs/model-evoformer-pair2.png]]

*** EvoFormer: Triangular Multiplicative Update cite:jumperHighlyAccurateProtein2021
#+ATTR_LATEX: width=\textwidth
[[./imgs/triangular-mult-update.png]]

*** EvoFormer: Triangular Self-Attention cite:jumperHighlyAccurateProtein2021
#+ATTR_LATEX: width=\textwidth
[[./imgs/triangular-self-attention.png]]

** Structure Module
*** Structure Module: Overview cite:jumperHighlyAccurateProtein2021
#+ATTR_LATEX: width=\textwidth
[[./imgs/model-structure.png]]

*** Structure Module: Frame Representation 

#+ATTR_LATEX: height=\textheight :caption {Example transform cite:SpatialTransformationMatrices}
[[./imgs/TransformationMatrix1.png]]

- rotation + translation transforms $T_i := (R_i,t_i)$
- no reflection, scaling, or shear
- they construct ground truth frames using the position of three atoms from the ground truth PDB structures using a Gram–Schmidt process (Algorithm 21)
*** Structure Module: IPA cite:jumperHighlyAccurateProtein2021
#+ATTR_LATEX: width=\textwidth
[[./imgs/ipa.png]]

*** Structure Module: Algorithm Part 1 cite:jumperHighlyAccurateProtein2021
#+ATTR_LATEX: width=\textwidth
[[./imgs/algo20-part1.png]]

*** Structure Module: Algorithm Part 2 cite:jumperHighlyAccurateProtein2021
#+ATTR_LATEX: width=\textwidth
[[./imgs/algo20-part2.png]]

*** Structure Module: Algorithm Part 3 cite:jumperHighlyAccurateProtein2021
#+ATTR_LATEX: width=\textwidth
[[./imgs/algo20-part3.png]]

*** Structure Module: Output :ignore:noexport:ARCHIVE:

- predicts backbone frames $T_i$ and torsion angles $α^f_i$
- then computes atom coordinates by applying the torsion angles to the corresponding amino acid structure with idealized bond angles and bond lengths.
- We attach a local frame to each rigid group (see Table 2), such that the torsion axis is the x-axis, and store the ideal literature atom coordinates [97] for each amino acid relative to these frames
in a table ~xlit
r,f,a , where r ∈ {ALA, ARG, ASN, . . . } denotes the residue type, f ∈ Storsion names denotes the
frame and a the atom name. We further pre-compute rigid transformations that transform atom coordinates
lit
from each frame to the frame that is higher up in the hierarchy. E.g. Tr,(χ
maps atoms in amino-acid
2 →χ1 )
type r from the χ2 -frame to the χ1 -frame. As we are only predicting heavy atoms, the extra backbone rigid
groups ω and φ do not contain atoms, but the corresponding frames contribute to the FAPE loss for alignment
to the ground truth (like all other frames).
cite:jumperHighlyAccurateProtein2021

** Final
*** Loss Functions  cite:jumperHighlyAccurateProtein2021
#+ATTR_LATEX: width=\textwidth
[[./imgs/loss-eq.png]]

- weighted sum
- weighted to reduce importance of short sequences
  
*** Loss Functions & Auxillary Heads cite:jumperHighlyAccurateProtein2021
1. Side chain and backbone torsion angle loss
2. Frame aligned point error (FAPE)
   * Configurations with FAPE(X,Y) = 0
   * Metric properties of FAPE
3. Chiral properties of AlphaFold and its loss
4. Model confidence prediction (pLDDT)
5. TM-score prediction
6. Distogram prediction
7. Masked MSA prediction
8. "Experimentally resolved" prediction
9. Structural violations

*** Loss Functions: FAPE 

#+ATTR_LATEX: width=\textwidth :caption {Algorithm 28 cite:jumperHighlyAccurateProtein2021}
[[./imgs/fape-algo.png]]


- Variation of commonly used root-mean-squared deviation (RMSD) of atomic positions
- not invariant to reflections, preventing proteins of the wrong chirality. cite:rubieraAlphaFoldHereWhat, cite:jumperHighlyAccurateProtein2021
  
* AlphaFold Inference
*** AlphaFold Inference cite:jumperHighlyAccurateProtein2021

- AlphaFold receives input features derived from:
  - the amino-acid sequence
  - MSA
  - templates (see subsubsection 1.2.9)
- outputs features:
  - atom coordinates
  - the distogram
  - per-residue confidence scores.
- Recycling x3
  - initial recycled inputs are zero

Algorithm 2 outlines the main steps (see also Fig 1e and the corresponding description in the main article).

*** AlphaFold Training cite:jumperHighlyAccurateProtein2021
[[./imgs/af-training-table.png]]

* Results 
*** Results cite:jumperHighlyAccurateProtein2021
They did well

*** Results cite:jumperHighlyAccurateProtein2021
They did well

*** Results: Positional Encodings cite:jumperHighlyAccurateProtein2021

*** Novel Folds
They did well

*** Ablation Studies cite:jumperHighlyAccurateProtein2021 
Baseline for all ablation models: Full model without noisy-student self-attention  
Ablations:
1. With noisy-student self-distillation training
2. No templates
3. No raw MSA (use MSA pairwise frequencies)
4. No triangles, biasing, or gating (use axial attention)
5. No recycling
6. No IPA (use direct projection)
7. No invariant IPA & no recycling
8. No end-to-end structure gradients (keep auxiliary heads)
9. No auxiliary distogram head
10. No auxiliary masked MSA head

*** Network Probing cite:jumperHighlyAccurateProtein2021 
todo

*** Attention Visualization cite:jumperHighlyAccurateProtein2021
todo

* Algorithms
*** Algorithm 23 Backbone update cite:jumperHighlyAccurateProtein2021 
#+ATTR_LATEX: width=\textwidth
[[./imgs/backbone-update-algo.png]]
*** Algorithm 24 Compute all atom coordinates cite:jumperHighlyAccurateProtein2021 
#+ATTR_LATEX: height=0.9\textheight :caption 
[[./imgs/all-atom-coords-algo.png]]
*** Algorithm 25 Make a transformation that rotates around the x-axis cite:jumperHighlyAccurateProtein2021 
#+ATTR_LATEX: width=\textwidth
[[./imgs/xaxis-transform-algo.png]]
*** Algorithm 26 Rename symmetric ground truth atoms cite:jumperHighlyAccurateProtein2021 
#+ATTR_LATEX: width=\textwidth
[[./imgs/rename-truth-atoms-algo26.png]]
*** Algorithm 27 Side chain and backbone torsion angle loss cite:jumperHighlyAccurateProtein2021 
#+ATTR_LATEX: width=\textwidth
[[./imgs/sidechain-backbonetorsion-loss-algo27.png]]
*** Algorithm 29 Predict model confidence pLDDT cite:jumperHighlyAccurateProtein2021 
#+ATTR_LATEX: width=\textwidth
[[./imgs/confidence-pLDDT-algo29.png]]
*** Algorithm 30 Generic recycling inference procedure cite:jumperHighlyAccurateProtein2021  
#+ATTR_LATEX: width=\textwidth
[[./imgs/recycling-algo30.png]]
*** Algorithm 31 Generic recycling training procedure cite:jumperHighlyAccurateProtein2021  
#+ATTR_LATEX: height=\textheight
[[./imgs/generic-recycling-algo31.png]]
*** Algorithm 32 Embedding of evoformer and structure module outputs for recycling cite:jumperHighlyAccurateProtein2021 
#+ATTR_LATEX: height=\textheight
[[./imgs/recycling-embedding-algo32.png]]
*** Distograms cite:jumperHighlyAccurateProtein2021
#+ATTR_LATEX: :height \textheight
[[./imgs/Examples-of-distograms-from-trRosetta.jpg]]
* Biblio
*** Bibliography
  :PROPERTIES:
  :BEAMER_OPT: fragile,allowframebreaks,label=
  :END:      
  
\printbibliography
